{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48e1e988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark-3.4.0-bin-hadoop3'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14279733",
   "metadata": {},
   "source": [
    "findspark\n",
    "\n",
    "1. **`findspark`** is a Python library that helps find the location of Spark installed in your system and makes it available in the Python environment. However, **`findspark`** is specifically designed to work with Apache Spark, not with other frameworks or tools.\n",
    "\n",
    "2. We need **``findspark``** to load pyspark package. We need to use findspark package to make a Spark Context available in your code.\n",
    "\n",
    "3. If you are using Spark and want to use **``findspark``** to initialize it in your Python environment. ``find.init()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a06d5c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef10019d",
   "metadata": {},
   "source": [
    "The **``SparkContext``** is the entry point for any Spark functionality and represents the connection to a Spark cluster. It allows you to create RDDs (Resilient Distributed Datasets) and perform various operations on them.\n",
    "\n",
    "Here are some common use cases and functionalities of **``SparkContext``**:\n",
    "1. Creating RDDs\n",
    "2. Data Transformations\n",
    "3. Actions\n",
    "\n",
    "It's worth noting that in newer versions of Spark, it is recommended to use **``SparkSession``** instead of directly creating a **``SparkContext``**. **``SparkSession``** provides a higher-level interface and encapsulates the functionality of **``SparkContext``** along with additional features for working with structured data using DataFrames and Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3a704b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201193e8",
   "metadata": {},
   "source": [
    "**``SparkSession``** consolidates various functionalities previously provided by SparkContext, SQLContext, and HiveContext.\n",
    "\n",
    "**``SparkSession``** is the entry point for working with structured data in Spark. It provides methods to configure and create a Spark application. In the code snippet, the appName method is used to set the name of the Spark application to \"Join Text Files with Condition\". This name will be displayed in the Spark UI and logs.\n",
    "\n",
    "The getOrCreate method is called to either create a new **``SparkSession``** or get an existing one if it already exists. This ensures that only one **``SparkSession``** is created per application, which is the recommended practice.\n",
    "\n",
    "Once the spark object is created, you can use it to perform various operations on structured data using Spark SQL and the DataFrame API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94cbd423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Join Text Files with Condition\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee954c8d",
   "metadata": {},
   "source": [
    "The code snippet reads two CSV files, `input2.txt` and `input1.txt`, using the **`spark.read.csv`** method. The options **`header`** and **`inferSchema`** are set to true to indicate that the CSV files have a header row and to infer the schema of the data, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "91860be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewership = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(r\"C:\\Users\\Gurudeep\\Downloads\\Team 12\\Team 12\\input2.txt\")\n",
    "shows = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(r\"C:\\Users\\Gurudeep\\Downloads\\Team 12\\Team 12\\input1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b2c34e",
   "metadata": {},
   "source": [
    "To display the first 10 rows of the shows DataFrame in Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54b74266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+\n",
      "|              Show|Channel|\n",
      "+------------------+-------+\n",
      "|     Hourly_Sports|    DEF|\n",
      "|        Baked_News|    BAT|\n",
      "|PostModern_Talking|    XYZ|\n",
      "|         Loud_News|    CNO|\n",
      "|       Almost_Show|    ABC|\n",
      "|       Hot_Talking|    DEF|\n",
      "|         Dumb_Show|    BAT|\n",
      "|      Surreal_Show|    XYZ|\n",
      "|      Cold_Talking|    CNO|\n",
      "|    Hourly_Cooking|    ABC|\n",
      "+------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shows.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b7b8ab51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+\n",
      "|       ShowName|Viewer|\n",
      "+---------------+------+\n",
      "|  Hourly_Sports|    21|\n",
      "|PostModern_Show|    38|\n",
      "|   Surreal_News|    73|\n",
      "|   Dumb_Cooking|   144|\n",
      "|   Cold_Talking|   287|\n",
      "| Almost_Talking|   574|\n",
      "|      Loud_News|   113|\n",
      "|    Hot_Talking|   228|\n",
      "|    Baked_Games|   459|\n",
      "| Hourly_Talking|   922|\n",
      "+---------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "viewership.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383e5d14",
   "metadata": {},
   "source": [
    "To perform an inner join between the viewership and shows DataFrames in Apache Spark, based on the condition ``viewership.ShowName == shows.Show``\n",
    "\n",
    "This will create a new DataFrame `joined_df` that contains the result of the inner join operation between the two DataFrames. The **``join``** operation will match the rows where the `ShowName` column in viewership is equal to the `Show` column in shows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6698d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = viewership.join(shows,viewership.ShowName ==  shows.Show,\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cbed7c",
   "metadata": {},
   "source": [
    "Filter the joined data frame based on the network name \"ABC\" using **``filter``** operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "055fdff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = joined_df.filter(joined_df.Channel == \"ABC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1258a1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+--------------+-------+\n",
      "|      ShowName|Viewer|          Show|Channel|\n",
      "+--------------+------+--------------+-------+\n",
      "|  Surreal_News|    73|  Surreal_News|    ABC|\n",
      "|   Baked_Games|   459|   Baked_Games|    ABC|\n",
      "|Hourly_Talking|   922|Hourly_Talking|    ABC|\n",
      "|   Almost_Show|   677|   Almost_Show|    ABC|\n",
      "|   Hourly_Show|   633|   Hourly_Show|    ABC|\n",
      "+--------------+------+--------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eedfa0",
   "metadata": {},
   "source": [
    "Checking the type of filtered_df and things it contains and their respective datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9f94a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ShowName: string, Viewer: int, Show: string, Channel: string]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f79651",
   "metadata": {},
   "source": [
    "Select `ShowName` and `Viewer` from `filtered_df` and store it in `wanted_df` as we would like to work on these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f083346",
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_df = filtered_df.select(\"ShowName\", \"Viewer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d346faa0",
   "metadata": {},
   "source": [
    "We see we would like to sum the columnsof `Viewer` w.r.t `ShowName` in `wanted_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e747c52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|        ShowName|Viewer|\n",
      "+----------------+------+\n",
      "|    Surreal_News|    73|\n",
      "|     Baked_Games|   459|\n",
      "|  Hourly_Talking|   922|\n",
      "|     Almost_Show|   677|\n",
      "|     Hourly_Show|   633|\n",
      "|    Dumb_Talking|  1022|\n",
      "|     Cold_Sports|  1025|\n",
      "|      Loud_Games|  1047|\n",
      "| PostModern_News|   487|\n",
      "|       Dumb_Show|   985|\n",
      "|       Hot_Games|   621|\n",
      "|  Hourly_Talking|   503|\n",
      "|       Cold_News|   251|\n",
      "|     Almost_News|   538|\n",
      "|       Loud_Show|    77|\n",
      "|PostModern_Games|   777|\n",
      "|  Surreal_Sports|   560|\n",
      "|    Dumb_Talking|   127|\n",
      "|        Hot_Show|   631|\n",
      "|      Baked_News|   274|\n",
      "+----------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wanted_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5bfb29",
   "metadata": {},
   "source": [
    "**Note**: We initially didnt use `wanted_df` because we had encountered an **`Analysis Exception`** which could not be resolved. Hence we continued working with `filtered_df`. But which was resolved later after submission(due to our Big Data Classes started on working with dataframes and groupby operation on 17/05/2023.\n",
    "\n",
    "The code you provided performs a group by operation on the `wanted_df` DataFrame using the `\"ShowName\"` column and calculates the sum of the `\"Viewer\"` column for each group. It then renames the `\"ShowName\"` column to `\"Show\"` and selects the `\"Show\"` and `\"total_viewers\"` columns. Finally, it displays the resulting DataFrame using the `show()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cbaad209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+\n",
      "|            Show|total_viewers|\n",
      "+----------------+-------------+\n",
      "| PostModern_News|         9736|\n",
      "|     Almost_Show|         8532|\n",
      "|       Hot_Games|         8716|\n",
      "|       Dumb_Show|         9956|\n",
      "|      Baked_News|         7824|\n",
      "|  Hourly_Talking|        19704|\n",
      "|        Hot_Show|         8036|\n",
      "|    Dumb_Talking|        18704|\n",
      "|       Cold_News|         7500|\n",
      "|  Hourly_Cooking|         8452|\n",
      "|    Almost_Games|         8149|\n",
      "|      Loud_Games|        10304|\n",
      "|     Cold_Sports|         9636|\n",
      "|     Hourly_Show|         7992|\n",
      "|PostModern_Games|         8244|\n",
      "|     Baked_Games|         9692|\n",
      "|     Almost_News|         7596|\n",
      "|    Surreal_News|         8024|\n",
      "|       Loud_Show|         7804|\n",
      "|  Surreal_Sports|         8144|\n",
      "+----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "from pyspark.sql.functions import col\n",
    "total_viewers = wanted_df.groupBy(\"ShowName\").agg(sum(\"Viewer\").alias(\"total_viewers\"))\n",
    "total_viewers = total_viewers.withColumnRenamed(\"ShowName\", \"Show\").select(\"Show\", \"total_viewers\")\n",
    "total_viewers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d851c7",
   "metadata": {},
   "source": [
    "The only difference here is we dont rename the column `ShowName` to `Show`. This is the testing phase of the program before actually moving further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d7fcd1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+\n",
      "|        ShowName|total_viewers|\n",
      "+----------------+-------------+\n",
      "| PostModern_News|         9736|\n",
      "|     Almost_Show|         8532|\n",
      "|       Hot_Games|         8716|\n",
      "|       Dumb_Show|         9956|\n",
      "|      Baked_News|         7824|\n",
      "|  Hourly_Talking|        19704|\n",
      "|        Hot_Show|         8036|\n",
      "|    Dumb_Talking|        18704|\n",
      "|       Cold_News|         7500|\n",
      "|  Hourly_Cooking|         8452|\n",
      "|    Almost_Games|         8149|\n",
      "|      Loud_Games|        10304|\n",
      "|     Cold_Sports|         9636|\n",
      "|     Hourly_Show|         7992|\n",
      "|PostModern_Games|         8244|\n",
      "|     Baked_Games|         9692|\n",
      "|     Almost_News|         7596|\n",
      "|    Surreal_News|         8024|\n",
      "|       Loud_Show|         7804|\n",
      "|  Surreal_Sports|         8144|\n",
      "+----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_df = filtered_df.groupBy(\"ShowName\").agg(sum(\"Viewer\").alias(\"total_viewers\"))\n",
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "199542e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ShowName: string, total_viewers: bigint]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "884766b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ShowName: string, total_viewers: bigint]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae16be58",
   "metadata": {},
   "source": [
    "After learning how to typecast a float to int. Reason being for Storage Optimization(occupies less space) , Aggregation and Grouping (becomes easy and consistent), Data Consistency(avoids any unexpected behavior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "40816ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_viewers = total_viewers.withColumn(\"total_viewers\", col(\"total_viewers\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "818ed006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Show: string, total_viewers: int]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_viewers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec824f3",
   "metadata": {},
   "source": [
    "Below code is used to copy the contents of a dataframe in `total_viewers` to `grouped_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "941e9291",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = total_viewers.select(*total_viewers.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8d82cd",
   "metadata": {},
   "source": [
    "We created a dataframe in pandas to convert it into a form of tuple and store the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "94dd6c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9cf93e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a661c2e1",
   "metadata": {},
   "source": [
    "Below code is used to extract the values from the columns `\"ShowName\"` and `\"total_viewers\"` in the DataFrame `grouped_df` and store them in Python lists named `name_list_ShowName` and `name_list_total_viewers`, respectively.\n",
    "\n",
    "Here's how the code works:\n",
    "\n",
    "**``grouped_df.select(\"ShowName\")``**: This selects the column `\"ShowName\"` from the DataFrame `grouped_df`, resulting in a new DataFrame with only that column.\n",
    "**`.rdd`**: This converts the DataFrame into an RDD (Resilient Distributed Dataset), which is a fundamental data structure in PySpark for distributed processing.\n",
    "**``flatMap(lambda x: x)``**: This lambda function is applied to each element of the RDD, returning the same element as it is. It is essentially used here to extract the values from the RDD.\n",
    "**``.collect()``**: This collects all the elements from the RDD and returns them as a list in the driver program.\n",
    "\n",
    "By executing the above code, you will have two Python lists: `name_list_ShowName` containing the values from the `\"ShowName\"` column and `name_list_total_viewers` containing the values from the `\"total_viewers\"` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "43b0b5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list_ShowName = grouped_df.select(\"Show\").rdd.flatMap(lambda x: x).collect()\n",
    "name_list_total_viewers = grouped_df.select(\"total_viewers\").rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1b6f8f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ShowName'] = name_list_ShowName\n",
    "df['total_viewers'] = name_list_total_viewers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10444978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tuples for each row\n",
    "tuples = list(df.itertuples(index=False, name=None))\n",
    "\n",
    "# write tuples to text file\n",
    "with open('output.txt', 'w') as f:\n",
    "    for t in tuples:\n",
    "        line = ','.join(str(x) for x in t) + '\\n'\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b7aa42",
   "metadata": {},
   "source": [
    "Additionally we calculated Total number of viewers for shows on ABC: 192745"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6a99e06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of viewers for shows on ABC: 192745\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of viewers\n",
    "total_viewers = filtered_df.selectExpr(\"sum(Viewer) as total_viewers\").collect()[0][\"total_viewers\"]\n",
    "# Display the output\n",
    "print(f\"Total number of viewers for shows on ABC: {total_viewers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4cf1f8",
   "metadata": {},
   "source": [
    "By calling **``spark.stop()``**, you terminate the Spark application and free up any resources used by Spark, such as memory and CPU. It's good practice to always include **``spark.stop()``** at the end of your Spark code to clean up resources properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "77761bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d69561",
   "metadata": {},
   "source": [
    "Thank you"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
